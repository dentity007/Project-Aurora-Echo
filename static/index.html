<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Aurora Echo</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 24px;
        }
        button {
            background-color: #4CAF50;
            border: none;
            color: white;
            padding: 14px 28px;
            font-size: 16px;
            border-radius: 6px;
            cursor: pointer;
        }
        button:disabled {
            background-color: #9ac69b;
            cursor: not-allowed;
        }
        #status {
            margin-top: 12px;
            font-style: italic;
            color: #444;
        }
        textarea {
            width: 100%;
            min-height: 160px;
            margin-top: 12px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 16px;
        }
        th, td {
            padding: 8px;
            border: 1px solid #ddd;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <h1>Project Aurora Echo</h1>
    <p>Capture a five-second snippet, transcribe it in real time, and receive a structured summary with action items.</p>
    <button id="recordButton">Record &amp; Analyze</button>
    <div id="status">Status: idle</div>

    <h2>Live Transcript</h2>
    <p id="transcription"></p>

    <h2>Summary</h2>
    <textarea id="summary" readonly></textarea>

    <h2>Action Items</h2>
    <table>
        <thead>
            <tr>
                <th>Task</th>
                <th>Assignee</th>
                <th>Due</th>
            </tr>
        </thead>
        <tbody id="actionsBody"></tbody>
    </table>

    <script>
        const recordButton = document.getElementById('recordButton');
        const statusElement = document.getElementById('status');
        const transcriptionElement = document.getElementById('transcription');
        const summaryElement = document.getElementById('summary');
        const actionsBody = document.getElementById('actionsBody');

        const TARGET_SAMPLE_RATE = 16000;
        const RECORDING_DURATION_MS = 5000;

        let ws;
        let audioContext;
        let processorNode;
        let sourceNode;
        let recordingTimeout;

        function updateStatus(message) {
            statusElement.textContent = `Status: ${message}`;
        }

        function ensureWebSocket() {
            if (ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) {
                return ws;
            }
            ws = new WebSocket(`${location.protocol === 'https:' ? 'wss' : 'ws'}://${location.host}/ws`);
            ws.binaryType = 'arraybuffer';
            ws.onopen = () => updateStatus('connected');
            ws.onclose = () => updateStatus('disconnected');
            ws.onerror = (err) => console.error('WebSocket error', err);
            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                if (data.type === 'status') {
                    updateStatus(data.status || 'processing');
                } else if (data.type === 'partial_transcript') {
                    transcriptionElement.textContent += `${data.text}\n`;
                } else if (data.type === 'final') {
                    if (data.error) {
                        alert(data.error);
                    } else {
                        summaryElement.value = data.summary || '';
                        actionsBody.innerHTML = '';
                        (data.actions || []).forEach((action) => {
                            const row = actionsBody.insertRow();
                            row.insertCell(0).textContent = action.task || '';
                            row.insertCell(1).textContent = action.assignee || '';
                            row.insertCell(2).textContent = action.due || '';
                        });
                    }
                    recordButton.disabled = false;
                    recordButton.textContent = 'Record & Analyze';
                    if (!data.error) {
                        updateStatus('complete');
                    }
                }
            };
            return ws;
        }

        function waitForSocketOpen(socket) {
            if (socket.readyState === WebSocket.OPEN) {
                return Promise.resolve();
            }
            return new Promise((resolve, reject) => {
                const handleOpen = () => {
                    socket.removeEventListener('close', handleClose);
                    resolve();
                };
                const handleClose = () => {
                    socket.removeEventListener('open', handleOpen);
                    reject(new Error('WebSocket closed before opening'));
                };
                socket.addEventListener('open', handleOpen, { once: true });
                socket.addEventListener('close', handleClose, { once: true });
            });
        }

        function clearResults() {
            transcriptionElement.textContent = '';
            summaryElement.value = '';
            actionsBody.innerHTML = '';
        }

        async function startRecording() {
            const socket = ensureWebSocket();
            clearResults();
            recordButton.disabled = true;
            recordButton.textContent = 'Recording...';
            updateStatus('requesting microphone');

            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                    },
                });

                await waitForSocketOpen(socket);

                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                sourceNode = audioContext.createMediaStreamSource(stream);
                processorNode = audioContext.createScriptProcessor(4096, 1, 1);

                socket.send(JSON.stringify({ type: 'start', sampleRate: TARGET_SAMPLE_RATE }));
                updateStatus('recording');

                processorNode.onaudioprocess = (event) => {
                    const inputData = event.inputBuffer.getChannelData(0);
                    let pcmData;

                    if (audioContext.sampleRate !== TARGET_SAMPLE_RATE) {
                        const ratio = TARGET_SAMPLE_RATE / audioContext.sampleRate;
                        const newLength = Math.floor(inputData.length * ratio);
                        const resampled = new Float32Array(newLength);
                        for (let i = 0; i < newLength; i++) {
                            const sourceIndex = i / ratio;
                            const lowerIndex = Math.floor(sourceIndex);
                            const upperIndex = Math.min(lowerIndex + 1, inputData.length - 1);
                            const weight = sourceIndex - lowerIndex;
                            resampled[i] = inputData[lowerIndex] * (1 - weight) + inputData[upperIndex] * weight;
                        }
                        pcmData = resampled;
                    } else {
                        pcmData = inputData;
                    }

                    const int16Buffer = new Int16Array(pcmData.length);
                    for (let i = 0; i < pcmData.length; i++) {
                        const s = Math.max(-1, Math.min(1, pcmData[i]));
                        int16Buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
                    }

                    if (socket.readyState === WebSocket.OPEN) {
                        socket.send(int16Buffer.buffer);
                    }
                };

                sourceNode.connect(processorNode);
                processorNode.connect(audioContext.destination);

                recordingTimeout = setTimeout(() => stopRecording(stream), RECORDING_DURATION_MS);
            } catch (error) {
                console.error('Microphone error', error);
                alert('Unable to access the microphone. Please check your permissions.');
                recordButton.disabled = false;
                recordButton.textContent = 'Record & Analyze';
                updateStatus('error');
            }
        }

        function stopRecording(stream) {
            if (recordingTimeout) {
                clearTimeout(recordingTimeout);
                recordingTimeout = null;
            }
            if (processorNode && sourceNode) {
                processorNode.disconnect();
                sourceNode.disconnect();
            }
            processorNode = null;
            sourceNode = null;
            if (stream) {
                stream.getTracks().forEach((track) => track.stop());
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            recordButton.textContent = 'Processing...';
            updateStatus('uploading');
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: 'stop' }));
            }
        }

        recordButton.addEventListener('click', () => {
            startRecording();
        });
    </script>
</body>
</html>
